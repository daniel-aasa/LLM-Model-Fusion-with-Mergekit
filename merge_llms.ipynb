{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/arcee-ai/mergekit.git\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd mergekit","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -e .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merge_config = \"\"\"\nslices:\n- sources:\n  - model: teknium/OpenHermes-2.5-Mistral-7B\n    layer_range:\n    - 0\n    - 32\n  - model: Open-Orca/Mistral-7B-OpenOrca\n    layer_range:\n    - 0\n    - 32\nmerge_method: slerp\nbase_model: Open-Orca/Mistral-7B-OpenOrca\nparameters:\n  t:\n  - filter: self_attn\n    value:\n    - 0\n    - 0.5\n    - 0.3\n    - 0.7\n    - 1\n  - filter: mlp\n    value:\n    - 1\n    - 0.5\n    - 0.7\n    - 0.3\n    - 0\n  - value: 0.5\ndtype: bfloat16\n\"\"\"\n\nwith open('config.yaml', 'w') as f:\n    f.write(merge_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mergekit-yaml config.yaml ./merge --copy-tokenizer --allow-crimes --out-shard-size 1B --trust-remote-code","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"api = HfApi(token=\"YOUR_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"username = \"your_username\"\nMODEL_NAME = \"orca-teknium-merge\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"api.create_repo(\n    repo_id = f\"{MODEL_NAME}\",\n    repo_type=\"model\"\n)\n\napi.upload_folder(\n    repo_id = f\"{username}/{MODEL_NAME}\",\n    folder_path = \"merge\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nMODEL_NAME = \"your_username/orca-teknium-merge\"\n\n# Enable quantization with bitsandbytes\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model and tokenizer\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef generate_response(user_input):\n    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n\n### Instruction:\n{user_input}\n\n### Response:\n\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(\n        **inputs, \n        max_new_tokens=512, \n        num_beams=10, \n        early_stopping=True, \n        no_repeat_ngram_size=2\n    )\n    \n    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n    \n    return response\n\n# Chat loop\nif __name__ == \"__main__\":\n    print(\"Chat with the AI! Type 'exit' to quit.\")\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            print(\"Goodbye!\")\n            break\n        response = generate_response(user_input)\n        print(\"AI:\", response)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}